{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "with open(\"cnn_outputs.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X_train = data[\"train_outputs\"]\n",
    "y_train = data[\"train_labels\"]\n",
    "X_valid = data[\"valid_outputs\"]\n",
    "y_valid = data[\"valid_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Determine and visualize the distribution of the data\n",
    "\n",
    "def analyze_distribution(X, y, title):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(X.shape[1]):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        for class_label in np.unique(y):\n",
    "            sns.histplot(X[y == class_label, i], kde=True, label=f'Class {class_label}')\n",
    "        plt.title(f'Distribution of Feature {i+1} by Class')\n",
    "        plt.legend()\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for i in range(X.shape[1]):\n",
    "        _, p_value = stats.normaltest(X[:, i])\n",
    "        print(f\"Feature {i+1} normality test p-value: {p_value}\")\n",
    "\n",
    "analyze_distribution(X_train, y_train, \"Original Training Data Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create more data based on the distribution (only for training set)\n",
    "\n",
    "\n",
    "def create_more_data(X, y, multiplier=2):\n",
    "    new_X = []\n",
    "    new_y = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_X = X[y == class_label]\n",
    "        class_mean = np.mean(class_X, axis=0)\n",
    "        class_cov = np.cov(class_X.T)\n",
    "\n",
    "        new_samples = stats.multivariate_normal.rvs(\n",
    "            mean=class_mean, cov=class_cov, size=class_X.shape[0] * (multiplier - 1)\n",
    "        )\n",
    "        new_X.append(np.vstack((class_X, new_samples)))\n",
    "        new_y.extend([class_label] * (class_X.shape[0] * multiplier))\n",
    "\n",
    "    return np.vstack(new_X), np.array(new_y)\n",
    "\n",
    "\n",
    "X_train_augmented, y_train_augmented = create_more_data(X_train, y_train, multiplier=200)\n",
    "\n",
    "analyze_distribution(\n",
    "    X_train_augmented, y_train_augmented, \"Augmented Training Data Distribution\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare data for training\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_train_aug_scaled = scaler.fit_transform(X_train_augmented)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "X_valid_tensor = torch.FloatTensor(X_valid_scaled)\n",
    "y_valid_tensor = torch.LongTensor(y_valid)\n",
    "\n",
    "X_train_aug_tensor = torch.FloatTensor(X_train_aug_scaled)\n",
    "y_train_aug_tensor = torch.LongTensor(y_train_augmented)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=120, shuffle=True)\n",
    "\n",
    "train_aug_dataset = TensorDataset(X_train_aug_tensor, y_train_aug_tensor)\n",
    "train_aug_loader = DataLoader(train_aug_dataset, batch_size=120, shuffle=True)\n",
    "\n",
    "valid_dataset = TensorDataset(X_valid_tensor, y_valid_tensor)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=120, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define the model\n",
    "\n",
    "\n",
    "class H97_ANN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(H97_ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train the model with early stopping\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, valid_loader, criterion, optimizer, num_epochs=100, patience=10, model_name=\"best_model_H97_ANN.pth\"\n",
    "):\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "        valid_loss /= len(valid_loader)\n",
    "        valid_losses.append(valid_loss)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Valid Loss: {valid_loss:.4f}\"\n",
    "        )\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), model_name)\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve == patience:\n",
    "            print(\"Early stopping!\")\n",
    "            model.load_state_dict(torch.load(model_name))\n",
    "            break\n",
    "\n",
    "    return model, train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Train and evaluate models\n",
    "\n",
    "def train_and_evaluate(X_train, y_train, X_valid, y_valid, title):\n",
    "    model = H97_ANN(X_train.shape[1], 97, len(np.unique(y_train)))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    train_dataset = TensorDataset(torch.FloatTensor(X_train), torch.LongTensor(y_train))\n",
    "    train_loader = DataLoader(train_dataset, batch_size=120, shuffle=True)\n",
    "    valid_dataset = TensorDataset(torch.FloatTensor(X_valid), torch.LongTensor(y_valid))\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=120, shuffle=False)\n",
    "\n",
    "    model, train_losses, valid_losses = train_model(model, train_loader, valid_loader, criterion, optimizer, model_name=f\"best_model_H97_ANN_{title}.pth\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        valid_outputs = model(torch.FloatTensor(X_valid))\n",
    "        _, predicted = torch.max(valid_outputs.data, 1)\n",
    "        accuracy = (predicted == torch.LongTensor(y_valid)).float().mean()\n",
    "    \n",
    "    print(f\"{title} - Validation accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # Visualize predictions\n",
    "    y_pred = nn.Softmax(dim=1)(valid_outputs).numpy()\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(3):\n",
    "        for j in range(3):\n",
    "            if i != j:\n",
    "                plt.subplot(3, 3, i*3 + j + 1)\n",
    "                scatter = plt.scatter(y_pred[:, i], y_pred[:, j], c=y_valid, cmap='viridis', alpha=0.5)\n",
    "                plt.xlabel(f'Class {i} probability')\n",
    "                plt.ylabel(f'Class {j} probability')\n",
    "    plt.colorbar(scatter)\n",
    "    plt.suptitle(f'{title} - Prediction Probabilities')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "    plt.title(f'{title} - Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate model without augmentation\n",
    "train_and_evaluate(X_train_scaled, y_train, X_valid_scaled, y_valid, \"Model_without_Augmentation\")\n",
    "\n",
    "# Train and evaluate model with augmentation\n",
    "train_and_evaluate(X_train_aug_scaled, y_train_augmented, X_valid_scaled, y_valid, \"Model_with_Augmentation\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
