{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data for train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to_ann_39 & to_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "\n",
    "# Đọc CSV\n",
    "df = pd.read_csv('/mnt/DataSamsung/project/Research_ThyroidFNA_ClassAI/phase3_140924/data/temp/test_not_aug.csv')\n",
    "\n",
    "# Hàm chuyển predicted_vector từ string thành list\n",
    "def to_ann_39(vec_str):\n",
    "    return np.array(ast.literal_eval(vec_str)).flatten()\n",
    "\n",
    "def to_transformer(vec_str):\n",
    "    return np.array(ast.literal_eval(vec_str))\n",
    "\n",
    "# Áp dụng hàm cho từng bản ghi để chuyển đổi predicted_vector từ string thành numpy array\n",
    "df['to_ann_39'] = df['predicted_vector'].apply(to_ann_39)\n",
    "df['to_transformer'] = df['predicted_vector'].apply(to_transformer)\n",
    "\n",
    "print(type(df['to_ann_39'][0]))      # numpy.ndarray\n",
    "print(df['to_ann_39'][0].shape)      # (39,)\n",
    "\n",
    "print(type(df['to_transformer'][0]))   # numpy.ndarray\n",
    "print(df['to_transformer'][0].shape)   # (13, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['to_ann_39'][0])\n",
    "print(df['to_transformer'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reduce dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize dữ liệu\n",
    "X = np.vstack(df['to_ann_39'].values)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Sử dụng PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Tính variance explained ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Elbow Method\n",
    "plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance), marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Elbow Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Sử dụng PCA giữ lại 95% thông tin\n",
    "pca_95 = PCA(0.95)\n",
    "X_pca_95 = pca_95.fit_transform(X_scaled)\n",
    "\n",
    "# In ra số chiều sau khi giảm\n",
    "print(f'Số chiều sau khi giảm: {X_pca_95.shape[1]}')\n",
    "\n",
    "# Lưu kết quả PCA vào cột mới trong DataFrame\n",
    "df['pca95_to_ann'] = list(X_pca_95)\n",
    "\n",
    "# Lưu cấu trúc PCA để áp dụng cho tập val và test\n",
    "with open('pca_95.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_95, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chọn số chiều phù hợp từ Elbow Method\n",
    "n_components_optimal = np.argmax(np.cumsum(explained_variance) >= 0.90) + 1  # Ví dụ chọn số chiều khi explained variance đạt 90%\n",
    "\n",
    "print(f'Số chiều tối ưu: {n_components_optimal}')\n",
    "\n",
    "# Sử dụng PCA với số chiều từ Elbow Method\n",
    "pca_elbow = PCA(n_components=n_components_optimal)\n",
    "X_pca_elbow = pca_elbow.fit_transform(X_scaled)\n",
    "\n",
    "# Lưu kết quả PCA từ Elbow Method vào cột mới\n",
    "df['pcaelbow_to_ann'] = list(X_pca_elbow)\n",
    "\n",
    "# Lưu cấu trúc PCA từ Elbow Method để áp dụng cho tập val và test\n",
    "with open('pca_elbow.pkl', 'wb') as f:\n",
    "    pickle.dump(pca_elbow, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sử dụng PLSRegression để giảm chiều về 2\n",
    "pls = PLSRegression(n_components=2)\n",
    "X_pls = pls.fit_transform(X_scaled, df['label'])[0]\n",
    "\n",
    "# Lưu kết quả PLS vào DataFrame\n",
    "df['pls_to_ann'] = list(X_pls)\n",
    "\n",
    "# Trực quan hóa dữ liệu sau khi giảm chiều\n",
    "# Định nghĩa colormap cho 3 nhãn\n",
    "cmap = ListedColormap(['r', 'g', 'b'])  # Đỏ cho nhãn 0, xanh lá cho nhãn 1, xanh dương cho nhãn 2\n",
    "\n",
    "# Trực quan hóa dữ liệu sau khi giảm chiều\n",
    "plt.figure(figsize=(8, 6))\n",
    "scatter = plt.scatter(X_pls[:, 0], X_pls[:, 1], c=df['label'], cmap=cmap, edgecolor='k')\n",
    "\n",
    "# Gắn nhãn cho trục và tiêu đề\n",
    "plt.xlabel('PLS Component 1')\n",
    "plt.ylabel('PLS Component 2')\n",
    "plt.title('PLS Reduced Data Visualization')\n",
    "\n",
    "# Tạo colorbar với các nhãn\n",
    "cbar = plt.colorbar(scatter, ticks=[0, 1, 2])\n",
    "cbar.set_label('Label')\n",
    "cbar.set_ticks([0, 1, 2])  # Thiết lập chính xác 3 giá trị nhãn\n",
    "cbar.set_ticklabels(['Class 0', 'Class 1', 'Class 2'])\n",
    "\n",
    "# Hiển thị biểu đồ\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['path_to_image', 'predicted_vector', 'predicted_label'], inplace=True)\n",
    "df.head()\n",
    "\n",
    "# Sau khi có thêm PLS, lưu lại DataFrame thành CSV\n",
    "df.to_csv('train_set_processed_data_with_pca_pls.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing data for val & test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('/mnt/DataSamsung/project/Research_ThyroidFNA_ClassAI/phase3_140924/data/temp/val_not_aug.csv')\n",
    "df_test = pd.read_csv('/mnt/DataSamsung/project/Research_ThyroidFNA_ClassAI/phase3_140924/data/temp/test_not_aug.csv')\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load lại PCA đã lưu\n",
    "with open('pca_95.pkl', 'rb') as f:\n",
    "    pca_95_loaded = pickle.load(f)\n",
    "\n",
    "with open('pca_elbow.pkl', 'rb') as f:\n",
    "    pca_elbow_loaded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More work to do here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cross_decomposition import PLSRegression\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import matplotlib.pyplot as plt\n",
    "# import joblib\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# # Đọc dữ liệu từ train set\n",
    "# df_train = pd.read_csv(\"/mnt/DataSamsung/project/Research_ThyroidFNA_ClassAI/phase3_140924/data/temp/train_not_aug.csv\")\n",
    "# df_val = pd.read_csv(\"/mnt/DataSamsung/project/Research_ThyroidFNA_ClassAI/phase3_140924/data/temp/val_not_aug.csv\")\n",
    "# df_test = pd.read_csv(\"/mnt/DataSamsung/project/Research_ThyroidFNA_ClassAI/phase3_140924/data/temp/test_not_aug.csv\")\n",
    "\n",
    "# # Chuyển đổi mỗi chuỗi thành mảng numpy\n",
    "# df_train['predicted_vector'] = df_train['predicted_vector'].apply(lambda x: np.array(eval(x)))\n",
    "# df_val['predicted_vector'] = df_val['predicted_vector'].apply(lambda x: np.array(eval(x)))\n",
    "# df_test['predicted_vector'] = df_test['predicted_vector'].apply(lambda x: np.array(eval(x)))\n",
    "\n",
    "# # Scale dữ liệu trước khi áp dụng PCA/PLS\n",
    "# X_train_scaled = df_train['predicted_vector']\n",
    "# X_val_scaled = df_val['predicted_vector']\n",
    "# X_test_scaled = df_test['predicted_vector']\n",
    "\n",
    "# # PCA giữ 95% thông tin\n",
    "# pca_95 = PCA(0.99)  # n_components=0.95\n",
    "# X_train_pca95 = pca_95.fit_transform(X_train_scaled)\n",
    "# X_val_pca95 = pca_95.transform(X_val_scaled)\n",
    "# X_test_pca95 = pca_95.transform(X_test_scaled)\n",
    "\n",
    "# # Lưu PCA 95 để sử dụng sau này\n",
    "# joblib.dump(pca_95, './pca_95.pkl')\n",
    "\n",
    "# # Lưu lại PCA này và thêm vào df_train, df_val, df_test\n",
    "# df_train['pca95_to_ann'] = X_train_pca95.tolist()\n",
    "# df_val['pca95_to_ann'] = X_val_pca95.tolist()\n",
    "# df_test['pca95_to_ann'] = X_test_pca95.tolist()\n",
    "\n",
    "# # PCA bằng elbow method\n",
    "# pca = PCA()\n",
    "# pca.fit(X_train_scaled)\n",
    "# # Tính variance explained ratio\n",
    "# explained_variance = pca.explained_variance_ratio_\n",
    "# # Elbow Method\n",
    "# plt.plot(range(1, len(explained_variance)+1), np.cumsum(explained_variance), marker='o', linestyle='--')\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.ylabel('Cumulative Explained Variance')\n",
    "# plt.title('PCA Elbow Method')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca_elbow = PCA(n_components=5)  # Chọn số chiều từ elbow method đã xác định trước\n",
    "# X_train_pca_elbow = pca_elbow.fit_transform(X_train_scaled)\n",
    "# X_val_pca_elbow = pca_elbow.transform(X_val_scaled)\n",
    "# X_test_pca_elbow = pca_elbow.transform(X_test_scaled)\n",
    "\n",
    "# # Lưu PCA elbow để sử dụng sau này\n",
    "# joblib.dump(pca_elbow, './pca_elbow.pkl')\n",
    "\n",
    "# # Lưu lại PCA này và thêm vào df_train, df_val, df_test\n",
    "# df_train['pcaelbow_to_ann'] = X_train_pca_elbow.tolist()\n",
    "# df_val['pcaelbow_to_ann'] = X_val_pca_elbow.tolist()\n",
    "# df_test['pcaelbow_to_ann'] = X_test_pca_elbow.tolist()\n",
    "\n",
    "# # PLS Regression để giảm về 2 chiều\n",
    "# pls = PLSRegression(n_components=2)\n",
    "# X_train_pls = pls.fit_transform(X_train_scaled, df_train['label'])[0]\n",
    "# X_val_pls = pls.transform(X_val_scaled)\n",
    "# X_test_pls = pls.transform(X_test_scaled)\n",
    "\n",
    "# # Lưu PLS model để sử dụng sau này\n",
    "# joblib.dump(pls, './pls_model.pkl')\n",
    "\n",
    "# # Trực quan hóa tập train sau khi PLS\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# plt.scatter(X_train_pls[:, 0], X_train_pls[:, 1], c=df_train['label'], cmap='viridis', edgecolor='k')\n",
    "# plt.xlabel('PLS Component 1')\n",
    "# plt.ylabel('PLS Component 2')\n",
    "# plt.title('PLS Reduced Data Visualization (Train Set)')\n",
    "# # Thêm thanh màu (colorbar) và đảm bảo rằng chỉ có 3 nhãn (0, 1, 2)\n",
    "# cbar = plt.colorbar(scatter, ticks=[0, 1, 2])\n",
    "# cbar.set_label('Label')\n",
    "# plt.show()\n",
    "\n",
    "# # Lưu lại kết quả PLS vào dataframe\n",
    "# df_train['pls_to_ann'] = X_train_pls.tolist()\n",
    "# df_val['pls_to_ann'] = X_val_pls.tolist()\n",
    "# df_test['pls_to_ann'] = X_test_pls.tolist()\n",
    "\n",
    "# # Lưu file CSV cho train, val và test\n",
    "# df_train.to_csv(\"./train_set_with_pca_pls.csv\", index=False)\n",
    "# df_val.to_csv(\"./val_set_with_pca_pls.csv\", index=False)\n",
    "# df_test.to_csv(\"./test_set_with_pca_pls.csv\", index=False)\n",
    "\n",
    "# # Load lại dữ liệu để chuẩn bị cho deep learning\n",
    "# df_train = pd.read_csv(\"./train_set_with_pca_pls.csv\")\n",
    "# df_val = pd.read_csv(\"./val_set_with_pca_pls.csv\")\n",
    "# df_test = pd.read_csv(\"./test_set_with_pca_pls.csv\")\n",
    "\n",
    "# # Lựa chọn các trường để train mạng deep learning\n",
    "# X_train = df_train['pca95_to_ann'].apply(eval).tolist()\n",
    "# y_train = df_train['label'].values\n",
    "\n",
    "# X_val = df_val['pca95_to_ann'].apply(eval).tolist()\n",
    "# y_val = df_val['label'].values\n",
    "\n",
    "# X_test = df_test['pca95_to_ann'].apply(eval).tolist()\n",
    "# y_test = df_test['label'].values\n",
    "\n",
    "# # Bây giờ bạn có thể sử dụng X_train, y_train, X_val, y_val, X_test, y_test để train mô hình deep learning\n",
    "\n",
    "# # Load lại mô hình PCA/PLS đã lưu để sử dụng tiếp\n",
    "# scaler = joblib.load('./scaler.pkl')\n",
    "# pca_95 = joblib.load('./pca_95.pkl')\n",
    "# pca_elbow = joblib.load('./pca_elbow.pkl')\n",
    "# pls = joblib.load('./pls_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def process_data(train_file, val_file, test_file):\n",
    "    # Hàm chuyển predicted_vector từ string thành list\n",
    "    def to_ann_39(vec_str):\n",
    "        return np.array(ast.literal_eval(vec_str)).flatten()\n",
    "    \n",
    "    def to_transformer(vec_str):\n",
    "        return np.array(ast.literal_eval(vec_str))\n",
    "\n",
    "    # Đọc và xử lý các tập dữ liệu\n",
    "    datasets = {'train': train_file, 'val': val_file, 'test': test_file}\n",
    "    for key, file in datasets.items():\n",
    "        df = pd.read_csv(file)\n",
    "        df['to_ann_39'] = df['predicted_vector'].apply(to_ann_39)\n",
    "        df['to_transformer'] = df['predicted_vector'].apply(to_transformer)\n",
    "\n",
    "        # Lưu các mảng NumPy dưới dạng JSON\n",
    "        df['to_ann_39'] = df['to_ann_39'].apply(lambda x: json.dumps(x.tolist()))  # Lưu dưới dạng JSON\n",
    "        df['to_transformer'] = df['to_transformer'].apply(lambda x: json.dumps(x.tolist()))  # Lưu dưới dạng JSON\n",
    "\n",
    "        # Lưu lại các file csv với 2 cột label và feature tương ứng\n",
    "        df[['label', 'to_ann_39']].to_csv(f'{key}_to_ann_39.csv', index=False)\n",
    "        df[['label', 'to_transformer']].to_csv(f'{key}_to_transformer.csv', index=False)\n",
    "\n",
    "\n",
    "# Gọi hàm với các file CSV tương ứng\n",
    "import os\n",
    "os.chdir('../data/temp')\n",
    "# process_data('train_not_aug.csv', 'val_not_aug.csv', 'test_not_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "\n",
    "def load_data_to_ann39_for_pytorch(file_path):\n",
    "    # Đọc dữ liệu\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Chuyển đổi các chuỗi JSON thành numpy array\n",
    "    df['to_ann_39'] = df['to_ann_39'].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "    # Chuyển đổi sang PyTorch tensor\n",
    "    features = torch.tensor(np.vstack(df['to_ann_39'].values)).float()\n",
    "    labels = torch.tensor(df['label'].values).long()\n",
    "\n",
    "    return features, labels\n",
    "\n",
    "def load_data_to_transformer_for_pytorch(file_path):\n",
    "    # Đọc dữ liệu\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # Chuyển đổi các chuỗi JSON thành numpy array\n",
    "    df['to_transformer'] = df['to_transformer'].apply(lambda x: np.array(json.loads(x)))\n",
    "\n",
    "    # Chuyển đổi sang PyTorch tensor\n",
    "    features = torch.tensor(np.vstack(df['to_transformer'].values)).float()\n",
    "    labels = torch.tensor(df['label'].values).long()\n",
    "\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Định nghĩa mạng ANN\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_prob):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=dropout_prob)\n",
    "        # self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        # x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Hàm tính trọng số của các label cho loss function\n",
    "def compute_class_weights(labels):\n",
    "    unique_labels, counts = torch.unique(labels, return_counts=True)\n",
    "    class_weights = len(labels) / counts.float()\n",
    "    weight_dict = {label.item(): class_weights[i].item() for i, label in enumerate(unique_labels)}\n",
    "    weights = torch.tensor([weight_dict[label.item()] for label in unique_labels])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Hàm tính toán accuracy và F1 score\n",
    "def calculate_metrics(labels, outputs):\n",
    "    _, preds = torch.max(outputs, 1)\n",
    "    acc = torch.sum(preds == labels).item() / labels.size(0)\n",
    "    f1 = f1_score(labels.cpu().numpy(), preds.cpu().numpy(), average='macro')\n",
    "    return acc, f1\n",
    "\n",
    "# Cải tiến hàm train model\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, early_stop_patience=10):\n",
    "    model.to(device)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        all_train_labels = []\n",
    "        all_train_outputs = []\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            all_train_labels.append(labels)\n",
    "            all_train_outputs.append(outputs)\n",
    "\n",
    "        # Tính toán train accuracy và F1 score\n",
    "        all_train_labels = torch.cat(all_train_labels)\n",
    "        all_train_outputs = torch.cat(all_train_outputs)\n",
    "        train_acc, train_f1 = calculate_metrics(all_train_labels, all_train_outputs)\n",
    "\n",
    "        # Evaluate trên tập validation\n",
    "        val_loss, val_acc, val_f1 = evaluate_model(model, val_loader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.4f}, Train F1: {train_f1:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0  # Reset patience counter if validation loss decreases\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= early_stop_patience:\n",
    "            print(f\"Stopping early at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "# Hàm đánh giá mô hình với thêm val F1\n",
    "def evaluate_model(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in data_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_outputs.append(outputs)\n",
    "\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "\n",
    "    accuracy = correct / total\n",
    "    f1 = f1_score(all_labels.cpu().numpy(), torch.argmax(all_outputs, dim=1).cpu().numpy(), average='weighted')\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    \n",
    "    return avg_loss, accuracy, f1\n",
    "\n",
    "# Hàm test với thêm confusion matrix\n",
    "def test_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            all_labels.append(labels)\n",
    "            all_predictions.append(predicted)\n",
    "\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_predictions = torch.cat(all_predictions)\n",
    "\n",
    "    accuracy = torch.sum(all_predictions == all_labels).item() / len(all_labels)\n",
    "    f1 = f1_score(all_labels.cpu().numpy(), all_predictions.cpu().numpy(), average='weighted')\n",
    "\n",
    "    # Ma trận nhầm lẫn (confusion matrix)\n",
    "    cm = confusion_matrix(all_labels.cpu().numpy(), all_predictions.cpu().numpy())\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return accuracy, f1, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data cho các tập train, val, test (giả sử các hàm load_data_to_ann39_for_pytorch đã tồn tại)\n",
    "train_features, train_labels = load_data_to_ann39_for_pytorch('train_to_ann_39.csv')\n",
    "val_features, val_labels = load_data_to_ann39_for_pytorch('val_to_ann_39.csv')\n",
    "test_features, test_labels = load_data_to_ann39_for_pytorch('test_to_ann_39.csv')\n",
    "\n",
    "# Tạo TensorDataset và DataLoader cho tập train, val, test\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset = TensorDataset(val_features, val_labels)\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights=tensor([3.5751, 3.4293, 2.3327])\n",
      "Shape of class_weights=torch.Size([3])\n",
      "Epoch [1/100], Loss: 32.4340, Train Acc: 0.5206, Train F1: 0.5052, Val Loss: 0.2301, Val Acc: 0.9259, Val F1: 0.9263\n",
      "Epoch [2/100], Loss: 7.9401, Train Acc: 0.8344, Train F1: 0.8334, Val Loss: 0.2005, Val Acc: 0.9444, Val F1: 0.9447\n",
      "Epoch [3/100], Loss: 4.3801, Train Acc: 0.9097, Train F1: 0.9109, Val Loss: 0.2061, Val Acc: 0.9444, Val F1: 0.9447\n",
      "Epoch [4/100], Loss: 3.7682, Train Acc: 0.9184, Train F1: 0.9171, Val Loss: 0.2155, Val Acc: 0.9407, Val F1: 0.9411\n",
      "Epoch [5/100], Loss: 3.2146, Train Acc: 0.9247, Train F1: 0.9257, Val Loss: 0.2239, Val Acc: 0.9407, Val F1: 0.9411\n",
      "Epoch [6/100], Loss: 2.3462, Train Acc: 0.9525, Train F1: 0.9534, Val Loss: 0.2327, Val Acc: 0.9407, Val F1: 0.9411\n",
      "Epoch [7/100], Loss: 2.6675, Train Acc: 0.9445, Train F1: 0.9452, Val Loss: 0.2388, Val Acc: 0.9519, Val F1: 0.9520\n",
      "Epoch [8/100], Loss: 2.6650, Train Acc: 0.9398, Train F1: 0.9402, Val Loss: 0.2397, Val Acc: 0.9407, Val F1: 0.9411\n",
      "Epoch [9/100], Loss: 2.5362, Train Acc: 0.9485, Train F1: 0.9490, Val Loss: 0.2467, Val Acc: 0.9407, Val F1: 0.9411\n",
      "Epoch [10/100], Loss: 2.0862, Train Acc: 0.9548, Train F1: 0.9553, Val Loss: 0.2533, Val Acc: 0.9481, Val F1: 0.9483\n",
      "Epoch [11/100], Loss: 2.4796, Train Acc: 0.9461, Train F1: 0.9464, Val Loss: 0.2600, Val Acc: 0.9407, Val F1: 0.9411\n",
      "Epoch [12/100], Loss: 2.2044, Train Acc: 0.9493, Train F1: 0.9496, Val Loss: 0.2609, Val Acc: 0.9370, Val F1: 0.9374\n",
      "Stopping early at epoch 12\n",
      "Test Accuracy: 88.60%\n",
      "Test F1 Score: 0.8867\n",
      "Confusion Matrix:\n",
      "[[ 57   0   4]\n",
      " [  0  81  15]\n",
      " [  0  12 103]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8860294117647058,\n",
       " 0.8866754543482271,\n",
       " array([[ 57,   0,   4],\n",
       "        [  0,  81,  15],\n",
       "        [  0,  12, 103]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Khởi tạo model\n",
    "input_dim = train_features.shape[1]\n",
    "hidden_dim = 97\n",
    "output_dim = len(torch.unique(train_labels))  # Số lớp output tương ứng với số nhãn\n",
    "dropout_prob = 0.9\n",
    "\n",
    "model = ANN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout_prob=dropout_prob)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Tính trọng số cho loss function\n",
    "class_weights = compute_class_weights(train_labels)\n",
    "class_weights = class_weights.to(device)  # Đảm bảo trọng số được chuyển lên GPU nếu có\n",
    "print(f'class_weights={class_weights}\\nShape of class_weights={class_weights.shape}')\n",
    "\n",
    "# Loss function và optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  # Dùng CrossEntropyLoss với trọng số của nhãn\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Huấn luyện model\n",
    "num_epochs = 100\n",
    "device = torch.device(device)\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "test_model(model, test_loader, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16406, 3]) torch.Size([1262])\n",
      "torch.Size([3510, 3]) torch.Size([270])\n",
      "torch.Size([3536, 3]) torch.Size([272])\n"
     ]
    }
   ],
   "source": [
    "# Ví dụ load file train_to_transformer.csv\n",
    "train_features, train_labels = load_data_to_transformer_for_pytorch('train_to_transformer.csv')\n",
    "print(train_features.shape, train_labels.shape)\n",
    "\n",
    "# Ví dụ load file val_to_transformer.csv\n",
    "val_features, val_labels = load_data_to_transformer_for_pytorch('val_to_transformer.csv')\n",
    "print(val_features.shape, val_labels.shape)\n",
    "\n",
    "# Ví dụ load file test_to_transformer.csv\n",
    "test_features, test_labels = load_data_to_transformer_for_pytorch('test_to_transformer.csv')\n",
    "print(test_features.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1262, 13, 3]) torch.Size([270, 13, 3]) torch.Size([272, 13, 3])\n"
     ]
    }
   ],
   "source": [
    "train_features = train_features.view(-1, 13, 3)\n",
    "val_features = val_features.view(-1, 13, 3)\n",
    "test_features = test_features.view(-1, 13, 3)\n",
    "\n",
    "print(train_features.shape, val_features.shape, test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Tạo TensorDataset và DataLoader cho tập train, val, test\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(train_features, train_labels)\n",
    "val_dataset = TensorDataset(val_features, val_labels)\n",
    "test_dataset = TensorDataset(test_features, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 3, 9])\n"
     ]
    }
   ],
   "source": [
    "num_patches = 13\n",
    "num_classes = 3\n",
    "dim = 9\n",
    "\n",
    "a = torch.zeros(1, num_patches, num_classes, dim)\n",
    "print(a.shape)\n",
    "\n",
    "# shape of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[ 0.6676,  0.2531, -0.3114],\n",
      "        [ 0.3443, -0.0797, -0.2200]], grad_fn=<AddmmBackward0>)\n",
      "Output shape: torch.Size([2, 3])\n",
      "Total parameters: 3535\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/mnt/DataSamsung/project/Research_ThyroidFNA_ClassAI/phase3_140924/src')\n",
    "from models.module2.transformer import main, get_transformer_model\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weights=tensor([3.5751, 3.4293, 2.3327])\n",
      "Shape of class_weights=torch.Size([3])\n",
      "Epoch [1/100], Loss: 16.0506, Train Acc: 0.7710, Train F1: 0.7523, Val Loss: 0.7061, Val Acc: 0.9370, Val F1: 0.9370\n",
      "Epoch [2/100], Loss: 12.6495, Train Acc: 0.8914, Train F1: 0.8857, Val Loss: 0.5798, Val Acc: 0.9519, Val F1: 0.9522\n",
      "Epoch [3/100], Loss: 10.6413, Train Acc: 0.9279, Train F1: 0.9245, Val Loss: 0.4867, Val Acc: 0.9481, Val F1: 0.9484\n",
      "Epoch [4/100], Loss: 8.7148, Train Acc: 0.9263, Train F1: 0.9222, Val Loss: 0.4060, Val Acc: 0.9519, Val F1: 0.9521\n",
      "Epoch [5/100], Loss: 7.3521, Train Acc: 0.9263, Train F1: 0.9229, Val Loss: 0.3450, Val Acc: 0.9481, Val F1: 0.9485\n",
      "Epoch [6/100], Loss: 6.0393, Train Acc: 0.9445, Train F1: 0.9418, Val Loss: 0.3031, Val Acc: 0.9519, Val F1: 0.9522\n",
      "Epoch [7/100], Loss: 4.9801, Train Acc: 0.9540, Train F1: 0.9523, Val Loss: 0.2752, Val Acc: 0.9519, Val F1: 0.9522\n",
      "Epoch [8/100], Loss: 4.5536, Train Acc: 0.9564, Train F1: 0.9545, Val Loss: 0.2992, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Epoch [9/100], Loss: 3.8863, Train Acc: 0.9588, Train F1: 0.9555, Val Loss: 0.2628, Val Acc: 0.9481, Val F1: 0.9485\n",
      "Epoch [10/100], Loss: 3.6486, Train Acc: 0.9540, Train F1: 0.9519, Val Loss: 0.2780, Val Acc: 0.9407, Val F1: 0.9410\n",
      "Epoch [11/100], Loss: 3.3117, Train Acc: 0.9501, Train F1: 0.9470, Val Loss: 0.2857, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Epoch [12/100], Loss: 3.1312, Train Acc: 0.9580, Train F1: 0.9567, Val Loss: 0.2854, Val Acc: 0.9407, Val F1: 0.9410\n",
      "Epoch [13/100], Loss: 3.0065, Train Acc: 0.9580, Train F1: 0.9566, Val Loss: 0.2641, Val Acc: 0.9444, Val F1: 0.9449\n",
      "Epoch [14/100], Loss: 2.8243, Train Acc: 0.9588, Train F1: 0.9567, Val Loss: 0.2905, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Epoch [15/100], Loss: 2.3461, Train Acc: 0.9659, Train F1: 0.9644, Val Loss: 0.3140, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Epoch [16/100], Loss: 2.8818, Train Acc: 0.9485, Train F1: 0.9455, Val Loss: 0.2996, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Epoch [17/100], Loss: 2.5244, Train Acc: 0.9493, Train F1: 0.9467, Val Loss: 0.3115, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Epoch [18/100], Loss: 2.8149, Train Acc: 0.9525, Train F1: 0.9505, Val Loss: 0.2943, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Epoch [19/100], Loss: 2.6654, Train Acc: 0.9540, Train F1: 0.9517, Val Loss: 0.3399, Val Acc: 0.9370, Val F1: 0.9373\n",
      "Stopping early at epoch 19\n",
      "Test Accuracy: 89.71%\n",
      "Test F1 Score: 0.8977\n",
      "Confusion Matrix:\n",
      "[[ 58   2   1]\n",
      " [  0  84  12]\n",
      " [  0  13 102]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8970588235294118,\n",
       " 0.8976833719913304,\n",
       " array([[ 58,   2,   1],\n",
       "        [  0,  84,  12],\n",
       "        [  0,  13, 102]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = get_transformer_model()\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Tính trọng số cho loss function\n",
    "class_weights = compute_class_weights(train_labels)\n",
    "class_weights = class_weights.to(device)  # Đảm bảo trọng số được chuyển lên GPU nếu có\n",
    "print(f'class_weights={class_weights}\\nShape of class_weights={class_weights.shape}')\n",
    "\n",
    "# Loss function và optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)  # Dùng CrossEntropyLoss với trọng số của nhãn\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Huấn luyện model\n",
    "num_epochs = 100\n",
    "\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Đánh giá trên tập test\n",
    "test_model(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_work#311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
