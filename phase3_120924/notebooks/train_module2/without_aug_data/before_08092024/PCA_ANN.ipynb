{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup môi trường"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../../..\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu từ CSV\n",
    "data_dir = './data/processed/'\n",
    "# train_df = pd.read_csv(data_di?r + 'train_features.csv').drop(columns=['image_path'])\n",
    "valid_df = pd.read_csv(data_dir + 'valid_features.csv').drop(columns=['image_path'])\n",
    "test_df = pd.read_csv(data_dir + 'test_features.csv').drop(columns=['image_path'])\n",
    "\n",
    "# data_dir = './data/augmented/'\n",
    "# train_df_aug = pd.read_csv(data_dir + 'train_augmented_features.csv')\n",
    "\n",
    "# Xem cấu trúc của DataFrame\n",
    "print('Train DataFrame:')\n",
    "print(train_df.head(3))\n",
    "print('Valid DataFrame:')\n",
    "print(valid_df.head(3))\n",
    "print('Test DataFrame:')\n",
    "print(test_df.head(3))\n",
    "# print('Train Augmented DataFrame:')\n",
    "# print(train_df_aug.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tính toán số lượng mẫu của mỗi lớp\n",
    "class_counts = train_df['label'].value_counts()\n",
    "total_samples = len(train_df)\n",
    "class_weights = {cls: total_samples / (len(class_counts) * count) for cls, count in class_counts.items()}\n",
    "\n",
    "# In ra thông tin trọng số\n",
    "print(\"Class Weights (inverse frequency):\")\n",
    "print(class_weights)\n",
    "\n",
    "# Chuyển đổi trọng số nhãn thành tensor\n",
    "class_weights_tensor = torch.tensor(sorted(list(class_weights.values()), reverse=True), dtype=torch.float)\n",
    "print(class_weights_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install kneed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the data\n",
    "X_train_flat = train_df.iloc[:, 1:].values\n",
    "X_valid_flat = valid_df.iloc[:, 1:].values\n",
    "X_test_flat = test_df.iloc[:, 1:].values\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X_train_flat)\n",
    "# skip standardization as the data is already standardized with same range\n",
    "X_scaled = X_train_flat\n",
    "\n",
    "# Step 2: Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# Step 3: Calculate explained variance and cumulative explained variance\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Step 4: Create visualizations\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Plot 1: Scree plot\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, 'bo-')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "\n",
    "# Plot 2: Cumulative explained variance plot\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'ro-')\n",
    "plt.title('Cumulative Explained Variance Ratio')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "\n",
    "# Add lines for 80%, 90%, and 95% explained variance\n",
    "plt.axhline(y=0.8, color='g', linestyle='--', label='80% Explained Variance')\n",
    "plt.axhline(y=0.9, color='b', linestyle='--', label='90% Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Explained variance for each component (bar plot)\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio)\n",
    "plt.title('Explained Variance Ratio per Component')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "\n",
    "# Plot 4: Cumulative explained variance (area plot)\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, 'ro-')\n",
    "plt.fill_between(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio)\n",
    "plt.title('Cumulative Explained Variance Ratio (Area Plot)')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Step 5: Print additional information\n",
    "print(\"Total number of components:\", len(explained_variance_ratio))\n",
    "print(\"\\nExplained variance ratio for each component:\")\n",
    "for i, ratio in enumerate(explained_variance_ratio, 1):\n",
    "    print(f\"PC{i}: {ratio:.4f}\")\n",
    "\n",
    "print(\"\\nNumber of components needed to explain:\")\n",
    "for threshold in [0.7, 0.8, 0.9, 0.95, 0.99]:\n",
    "    n_components = np.where(cumulative_variance_ratio >= threshold)[0][0] + 1\n",
    "    print(f\"{threshold*100}% of variance: {n_components}\")\n",
    "\n",
    "# Step 6: Determine optimal number of components (elbow method)\n",
    "from kneed import KneeLocator\n",
    "n_components_range = range(1, len(explained_variance_ratio) + 1)\n",
    "kneedle = KneeLocator(n_components_range, explained_variance_ratio, curve='convex', direction='decreasing')\n",
    "print(f\"\\nOptimal number of components (elbow method): {kneedle.elbow}\")\n",
    "\n",
    "# Step 7: Transform the data using the optimal number of components\n",
    "optimal_n_components = kneedle.elbow\n",
    "pca_optimal = PCA(n_components=optimal_n_components)\n",
    "X_pca = pca_optimal.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nShape of the transformed data: {X_pca.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Apply PCA\n",
    "n_components = ...\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train_flat)\n",
    "\n",
    "# Transform the data\n",
    "X_train_pca = pca.transform(X_train_flat)\n",
    "X_valid_pca = pca.transform(X_valid_flat)\n",
    "X_test_pca = pca.transform(X_test_flat)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train_pca, dtype=torch.float32)\n",
    "y_train = torch.tensor(train_df['label'].values, dtype=torch.long)\n",
    "X_valid = torch.tensor(X_valid_pca, dtype=torch.float32)\n",
    "y_valid = torch.tensor(valid_df['label'].values, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test_pca, dtype=torch.float32)\n",
    "y_test = torch.tensor(test_df['label'].values, dtype=torch.long)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_valid.shape, y_valid.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "valid_dataset = TensorDataset(X_valid, y_valid)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=120, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=120, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=120, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huấn luyện mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.models.module2.ann import ANN\n",
    "\n",
    "model = ANN(input_dim=..., output_dim=3, hidden_dim=9, num_layers=3, dropout=0.5)\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "torch.manual_seed(97)\n",
    "\n",
    "model.to('cuda')\n",
    "criterion.to('cuda')\n",
    "\n",
    "num_epochs = 100\n",
    "best_valid_loss = float('inf')\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Đánh giá mô hình trên tập validation\n",
    "    model.eval()\n",
    "    valid_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Validation Loss: {valid_loss:.4f}\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model_PCA_ANN_withoutAug.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kiểm thử"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trên tập valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load mô hình tốt nhất\n",
    "model.load_state_dict(torch.load('best_model_PCA_ANN_withoutAug.pth', weights_only=True, map_location='cpu'))\n",
    "\n",
    "# Dự đoán trên tập validation\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_prob = []  # Lưu xác suất dự đoán cho AUC\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in valid_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        prob = F.softmax(outputs, dim=1)  # Chuyển đổi đầu ra thành xác suất\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_pred_prob.extend(prob.cpu().numpy())  # Lưu xác suất dự đoán\n",
    "\n",
    "# Chuyển đổi danh sách thành mảng NumPy\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_prob = np.array(y_pred_prob)\n",
    "\n",
    "# Tính toán confusion matrix và classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Tính toán AUC cho từng lớp\n",
    "try:\n",
    "    auc_scores = roc_auc_score(y_true, y_pred_prob, multi_class='ovr')\n",
    "    print(\"\\nAUC Scores for each class:\")\n",
    "    print(auc_scores)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Vẽ confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trên tập test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load mô hình tốt nhất\n",
    "model.load_state_dict(torch.load('best_model_PCA_ANN_withoutAug.pth', weights_only=True, map_location='cpu'))\n",
    "\n",
    "# Dự đoán trên tập validation\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "y_pred_prob = []  # Lưu xác suất dự đoán cho AUC\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "        outputs = model(inputs)\n",
    "        prob = F.softmax(outputs, dim=1)  # Chuyển đổi đầu ra thành xác suất\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        \n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())\n",
    "        y_pred_prob.extend(prob.cpu().numpy())  # Lưu xác suất dự đoán\n",
    "\n",
    "# Chuyển đổi danh sách thành mảng NumPy\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "y_pred_prob = np.array(y_pred_prob)\n",
    "\n",
    "# Tính toán confusion matrix và classification report\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "class_report = classification_report(y_true, y_pred)\n",
    "\n",
    "# Tính toán AUC cho từng lớp\n",
    "try:\n",
    "    auc_scores = roc_auc_score(y_true, y_pred_prob, multi_class='ovr')\n",
    "    print(\"\\nAUC Scores for each class:\")\n",
    "    print(auc_scores)\n",
    "except ValueError as e:\n",
    "    print(f\"ValueError: {e}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Vẽ confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1', 'Class 2'], yticklabels=['Class 0', 'Class 1', 'Class 2'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
